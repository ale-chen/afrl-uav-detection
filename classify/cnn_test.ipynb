{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_directory = \"E:\\\\UAV_DISTASIO_DATA\\\\X\\\\ESCAPE_FORMAT_ONECHANNEL\"\n",
    "\n",
    "label_spreadsheet = \"E:\\\\UAV_DISTASIO_DATA\\\\y\\\\escape_singleUAV_scenarios_cleaned.xlsx\"\n",
    "\n",
    "# Read the existing label spreadsheet\n",
    "df_labels = pd.read_excel(label_spreadsheet, header=None, names=[\"filename\", \"type\", \"motion\"])\n",
    "\n",
    "# Extract the unique identifiers (sA1r01) from the label filenames\n",
    "df_labels[\"identifier\"] = df_labels[\"filename\"].str.extract(r\"(sA\\d+r\\d+)\")\n",
    "\n",
    "wav_files = [file for file in os.listdir(wav_directory) if file.endswith(\".wav\")]\n",
    "\n",
    "# Create a new DataFrame to store the entries for each .wav file\n",
    "df_entries = pd.DataFrame(columns=[\"filename\", \"type\", \"motion\"])\n",
    "\n",
    "# Iterate over each .wav file\n",
    "for wav_file in wav_files:\n",
    "    # Extract the identifier (sA1r01) from the .wav filename\n",
    "    identifier = wav_file.split(\"-\")[0]\n",
    "    \n",
    "    try:\n",
    "        # Find the corresponding label in the label DataFrame\n",
    "        label_row = df_labels[df_labels[\"identifier\"] == identifier].iloc[0]\n",
    "        \n",
    "        # Create a new DataFrame for the current entry\n",
    "        entry_df = pd.DataFrame({\n",
    "            \"filename\": [wav_file],\n",
    "            \"type\": [label_row[\"type\"]],\n",
    "            \"motion\": [label_row[\"motion\"]]\n",
    "        })\n",
    "        \n",
    "        # Concatenate the new entry DataFrame with the existing DataFrame\n",
    "        df_entries = pd.concat([df_entries, entry_df], ignore_index=True)\n",
    "        \n",
    "    except IndexError:\n",
    "        print(f\"No corresponding label found for file: {wav_file}\")\n",
    "        continue\n",
    "\n",
    "# Save the new DataFrame to a new Excel spreadsheet\n",
    "output_spreadsheet = \"E:\\\\UAV_DISTASIO_DATA\\\\y\\\\UAV_chunk_labels.xlsx\"\n",
    "df_entries.to_excel(output_spreadsheet, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, excel_file, audio_dir, transform=None, target_transform=None):\n",
    "        self.df = pd.read_excel(excel_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.label_map = {1: \"Inspired Flight 1200\", 2: \"DJI Matrice 800\", 3: \"DJI Phantom 4 Pro v2\", 5: \"Phantom and Matrice\"}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_path = os.path.join(self.audio_dir, self.df.iloc[idx, 0])\n",
    "        label = self.df.iloc[idx, 1]\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample the waveform if necessary\n",
    "        if sample_rate != 44100:\n",
    "            waveform = torchaudio.transforms.Resample(sample_rate, 44100)(waveform)\n",
    "        \n",
    "        # Convert waveform to spectrogram\n",
    "        spectrogram = torchaudio.transforms.Spectrogram()(waveform)\n",
    "        \n",
    "        # Convert spectrogram to grayscale tensor\n",
    "        grayscale_spectrogram = spectrogram.mean(dim=0).unsqueeze(0)\n",
    "        \n",
    "        if self.transform:\n",
    "            grayscale_spectrogram = self.transform(grayscale_spectrogram)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return grayscale_spectrogram, torch.tensor(label)\n",
    "    \n",
    "    \n",
    "    def save_spectrograms_as_tensors(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for idx in range(len(self)):\n",
    "            spectrogram, label = self[idx]\n",
    "            \n",
    "            # Convert label to string\n",
    "            label_str = self.label_map.get(label.item(), f\"Unknown_{label.item()}\")\n",
    "            \n",
    "            # Generate filename\n",
    "            filename = f\"spectrogram_{idx}_{label_str}.pt\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Save the spectrogram tensor\n",
    "            torch.save(spectrogram, filepath)\n",
    "            \n",
    "            print(f\"Saved spectrogram tensor: {filepath}\")\n",
    "\n",
    "# Usage example\n",
    "excel_file = \"E:\\\\UAV_DISTASIO_DATA\\\\y\\\\UAV_chunk_labels.xlsx\"\n",
    "audio_dir = r\"E:\\UAV_DISTASIO_DATA\\X\\ESCAPE_FORMAT_ONECHANNEL\"\n",
    "\n",
    "# Define any additional transformations if needed\n",
    "transform = None\n",
    "target_transform = None\n",
    "\n",
    "# Usage example\n",
    "excel_file = \"E:\\\\UAV_DISTASIO_DATA\\\\y\\\\UAV_chunk_labels.xlsx\"\n",
    "audio_dir = r\"E:\\UAV_DISTASIO_DATA\\X\\ESCAPE_FORMAT_ONECHANNEL\"\n",
    "\n",
    "# Define any transformations if needed\n",
    "transform = None\n",
    "target_transform = None\n",
    "\n",
    "dataset = SpectrogramDataset(excel_file, audio_dir, transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(.8 * len(dataset))\n",
    "test_size = int(.75 * len(dataset) - train_size)\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_size = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 1103])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(400)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, (5, 10), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8, (5, 10), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((5, 10), stride=(1, 5)),\n",
    "\n",
    "            nn.Conv2d(8, 16, (10, 5), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, (10, 5), stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((10, 5), stride=(2, 3)),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(101 * 74 * 16, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.seq(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_seconds(seconds):\n",
    "\n",
    "    minutes = seconds // 60\n",
    "\n",
    "    hours = minutes // 60\n",
    "\n",
    "    days = hours // 24\n",
    "\n",
    "    return seconds % 60, minutes % 60, hours % 24, days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define model\n",
    "\n",
    "    model = CNN()\n",
    "\n",
    "    #model = LSTM()\n",
    "\n",
    "    # Cuda setup\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "\n",
    "    # Optimizer setup\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    # optimizer = SGD(\n",
    "    #     model.parameters(),\n",
    "    #     lr=0.01,\n",
    "    #     weight_decay=1e-6,\n",
    "    #     momentum=0.9,\n",
    "    #     nesterov=True\n",
    "    # )\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    # Number of epochs\n",
    "    num_epochs = 8\n",
    "\n",
    "\n",
    "\n",
    "    # Train or load model?\n",
    "    model.train()\n",
    "    train_model = True\n",
    "    print(\"Training model....\")\n",
    "    start = time.time()\n",
    "\n",
    "    if train_model:\n",
    "        # LSTM hidden layer\n",
    "        # hidden = model.init_hidden()\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            total_loss = 0\n",
    "            for image, label in tqdm(train_loader, leave=False):\n",
    "                image, label = image.to(device), label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                # CNN\n",
    "                probabilities = model(image)\n",
    "\n",
    "                loss = loss_fn(probabilities, label)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                pass\n",
    "\n",
    "            tqdm.write(f\"Epoch {epoch + 1}/{num_epochs} has loss {total_loss}\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "        # torch.save(model.state_dict(), \"mnist_lstm.pt\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        state = torch.load(\"mnist_cnn.pt\", map_location=torch.device(device))\n",
    "\n",
    "        # state = torch.load(\"mnist_lstm.pt\", map_location=torch.device(device))\n",
    "\n",
    "        model.load_state_dict(state)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    seconds, minutes, hours, days = split_seconds(end - start)\n",
    "\n",
    "    print(f\"Training Runtime: {int(days)}d {int(hours)}h {int(minutes)}m {seconds}s\")\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate model on test data\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Evaluating model....\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    num_test = 0\n",
    "\n",
    "    num_correct = 0\n",
    "\n",
    "    for image, label in tqdm(test_loader):\n",
    "\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        probabilities = model(image, hidden)\n",
    "\n",
    "        _, pred = probabilities.max(1)\n",
    "\n",
    "        num_test += label.size(0)\n",
    "\n",
    "        num_correct += pred.eq(label).sum().item()\n",
    "\n",
    "    print(f\"Test accuracy: {num_correct / num_test * 100}\")\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    seconds, minutes, hours, days = split_seconds(end - start)\n",
    "\n",
    "    print(f\"Training Runtime: {int(days)}d {int(hours)}h {int(minutes)}m {seconds}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 54.13 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#model = LSTM()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Cuda setup\u001b[39;00m\n\u001b[0;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Optimizer setup\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 54.13 GiB. GPU "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save spectrograms as tensors\n",
    "dataset.save_spectrograms_as_tensors(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
